{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "import requests\n",
    "from requests_html import HTMLSession\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from msedge.selenium_tools import Edge, EdgeOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(ticker):\n",
    "  \"This opens the edge driver with on a given url\"\n",
    "  #Create the urls\n",
    "  url_statistics = f'https://finance.yahoo.com/quote/{ticker}/key-statistics?p={ticker}'\n",
    "  url_financials = f'https://finance.yahoo.com/quote/{ticker}/financials?p={ticker}'\n",
    "  url_historical = f'https://finance.yahoo.com/quote/{ticker}/history?p={ticker}'\n",
    "\n",
    "\n",
    "  driver.get(url_financials)\n",
    "  time.sleep(3)\n",
    "\n",
    "  soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "  pattern = re.compile(r'\\s--\\sData\\s--\\s')\n",
    "  script_data = soup.find('script', text=pattern).contents[0]\n",
    "\n",
    "  start = script_data.find(\"context\")-2\n",
    "  end = script_data.find(\"root.YAHOO ||\")-2\n",
    "\n",
    "  #data = [json.loads(line) for line in open('extra.json','r')]\n",
    "  \n",
    "  json_data = json.loads(script_data[start:end])\n",
    "\n",
    "  json_data['context'].keys()\n",
    "\n",
    "  annual_is = json_data['context']['dispatcher']['stores']['QuoteSummaryStore']['incomeStatementHistory']['incomeStatementHistory']\n",
    "  quarterly_is = json_data['context']['dispatcher']['stores']['QuoteSummaryStore']['incomeStatementHistoryQuarterly']['incomeStatementHistory']\n",
    "\n",
    "  annual_cf = json_data['context']['dispatcher']['stores']['QuoteSummaryStore']['cashflowStatementHistory']['cashflowStatements']\n",
    "  quarterly_cf = json_data['context']['dispatcher']['stores']['QuoteSummaryStore']['cashflowStatementHistoryQuarterly']['cashflowStatements']\n",
    "\n",
    "  annual_bs = json_data['context']['dispatcher']['stores']['QuoteSummaryStore']['balanceSheetHistory']['balanceSheetStatements']\n",
    "  quarterly_bs = json_data['context']['dispatcher']['stores']['QuoteSummaryStore']['balanceSheetHistoryQuarterly']['balanceSheetStatements']\n",
    "\n",
    "  qis = pd.json_normalize(quarterly_is)\n",
    "  qis['ticker'] = ticker\n",
    "  qis['endDate'] = qis['endDate.fmt']\n",
    "  qcf = pd.json_normalize(quarterly_cf)\n",
    "  qcf['ticker'] = ticker\n",
    "  qbs = pd.json_normalize(quarterly_bs)\n",
    "  qbs['ticker'] = ticker\n",
    "\n",
    "\n",
    "  result = pd.merge(qis, qcf, on=[\"ticker\", \"endDate.raw\"])\n",
    "\n",
    "  end_result = pd.merge(result, qbs, on=[\"ticker\", \"endDate.raw\"])\n",
    "\n",
    "  return end_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_stats(ticker):\n",
    "\n",
    "    #Create the urls\n",
    "    url_statistics = f'https://finance.yahoo.com/quote/{ticker}/key-statistics?p={ticker}'\n",
    "    url_financials = f'https://finance.yahoo.com/quote/{ticker}/financials?p={ticker}'\n",
    "    url_historical = f'https://finance.yahoo.com/quote/{ticker}/history?p={ticker}'\n",
    "\n",
    "\n",
    "    driver.get(url_statistics)\n",
    "    time.sleep(3)\n",
    "\n",
    "    # button = driver.find_element_by_xpath('//*[@id=\"Col1-1-Financials-Proxy\"]/section/div[1]/div[2]/button/div/span')\n",
    "    # button.click()\n",
    "    # time.sleep(0.5)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    pattern = re.compile(r'\\s--\\sData\\s--\\s')\n",
    "    script_data = soup.find('script', text=pattern).contents[0]\n",
    "\n",
    "    start = script_data.find(\"context\")-2\n",
    "    end = script_data.find(\"root.YAHOO ||\")-2\n",
    "    \n",
    "    json_data = json.loads(script_data[start:end])\n",
    "\n",
    "    keys = json_data['context']['dispatcher']['stores']['QuoteTimeSeriesStore']['timeSeries'].keys()\n",
    "\n",
    "    #loop through all the keys in the timeSeries store\n",
    "    for key in keys:\n",
    "        #the keys are broken into two categories, annual and trailing (twelve months)\n",
    "        if (key[:9] =='quarterly'):\n",
    "            #copy the json data for the key\n",
    "            temp_json = json_data['context']['dispatcher']['stores']['QuoteTimeSeriesStore']['timeSeries'][key]\n",
    "            check_data = pd.json_normalize(temp_json)\n",
    "            if (not check_data.empty):\n",
    "                if (check_data['asOfDate'].isnull().values.any()):\n",
    "                    print('skipped ' + key)\n",
    "                else:\n",
    "                    collected_data = check_data['asOfDate']\n",
    "                    print('found a full date column')\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            print('skipped ' + key)\n",
    "\n",
    "    #loop through all the keys in the timeSeries store\n",
    "    for key in keys:\n",
    "        #the keys are broken into two categories, annual and trailing (twelve months)\n",
    "        if (key[:9] =='quarterly'):\n",
    "            #copy the json data for the key\n",
    "            temp_json = json_data['context']['dispatcher']['stores']['QuoteTimeSeriesStore']['timeSeries'][key]\n",
    "            temp_data = pd.json_normalize(temp_json)\n",
    "            #replace the generic value column name with the key name\n",
    "            if (not temp_data.empty):\n",
    "                temp_data = temp_data.rename(columns={'reportedValue.raw': key})\n",
    "                temp_data = temp_data[key]\n",
    "                print(f'about to merge ' + key + ' table')\n",
    "                collected_data = pd.merge(collected_data, temp_data, left_index=True, right_index=True)\n",
    "                print(f'succesfully added ' + key + ' table to the collection.')\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        else:\n",
    "            print('skipped ' + key)\n",
    "    collected_data['ticker'] = ticker\n",
    "    return (collected_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_income(ticker):\n",
    "\n",
    "    #Create the urls\n",
    "    url_statistics = f'https://finance.yahoo.com/quote/{ticker}/key-statistics?p={ticker}'\n",
    "    url_financials = f'https://finance.yahoo.com/quote/{ticker}/financials?p={ticker}'\n",
    "    url_historical = f'https://finance.yahoo.com/quote/{ticker}/history?p={ticker}'\n",
    "\n",
    "\n",
    "    driver.get(url_financials)\n",
    "    time.sleep(3)\n",
    "\n",
    "    #button = driver.find_elements_by_xpath(\"//*[@id=\"Col1-1-Financials-Proxy\"]/section/div[1]/div[2]/button/div\")\n",
    "    #'//div[@class=\"css-ais6tt\"]//button[3]'\n",
    "    #//*[@id=\"Col1-1-Financials-Proxy\"]/section/div[1]/div[2]/button/div/span\n",
    "\n",
    "    # button = driver.find_element_by_xpath('//*[@id=\"Col1-1-Financials-Proxy\"]/section/div[1]/div[2]/button/div/span')\n",
    "    # button.click()\n",
    "    # time.sleep(0.5)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    pattern = re.compile(r'\\s--\\sData\\s--\\s')\n",
    "    script_data = soup.find('script', text=pattern).contents[0]\n",
    "\n",
    "    start = script_data.find(\"context\")-2\n",
    "    end = script_data.find(\"root.YAHOO ||\")-2\n",
    "\n",
    "    #data = [json.loads(line) for line in open('extra.json','r')]\n",
    "    \n",
    "    json_data = json.loads(script_data[start:end])\n",
    "\n",
    "    keys = json_data['context']['dispatcher']['stores']['QuoteTimeSeriesStore']['timeSeries'].keys()\n",
    "\n",
    "    #loop through all the keys in the timeSeries store\n",
    "    for key in keys:\n",
    "        #the keys are broken into two categories, annual and trailing (twelve months)\n",
    "        if (key[:6] =='annual'):\n",
    "            #copy the json data for the key\n",
    "            temp_json = json_data['context']['dispatcher']['stores']['QuoteTimeSeriesStore']['timeSeries'][key]\n",
    "            check_data = pd.json_normalize(temp_json)\n",
    "            if (not check_data.empty):\n",
    "                if (check_data['asOfDate'].isnull().values.any()):\n",
    "                    print('skipped ' + key)\n",
    "                else:\n",
    "                    collected_data = check_data['asOfDate']\n",
    "                    print('found a full date column')\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            print('skipped ' + key)\n",
    "\n",
    "    #loop through all the keys in the timeSeries store\n",
    "    for key in keys:\n",
    "        #the keys are broken into two categories, annual and trailing (twelve months)\n",
    "        if (key[:6] =='annual'):\n",
    "            #copy the json data for the key\n",
    "            temp_json = json_data['context']['dispatcher']['stores']['QuoteTimeSeriesStore']['timeSeries'][key]\n",
    "            temp_data = pd.json_normalize(temp_json)\n",
    "            #replace the generic value column name with the key name\n",
    "            if (not temp_data.empty):\n",
    "                temp_data = temp_data.rename(columns={'reportedValue.raw': key})\n",
    "                temp_data = temp_data[key]\n",
    "                print(f'about to merge ' + key + ' table')\n",
    "                collected_data = pd.merge(collected_data, temp_data, left_index=True, right_index=True)\n",
    "                print(f'succesfully added ' + key + ' table to the collection.')\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        else:\n",
    "            print('skipped ' + key)\n",
    "    collected_data['ticker'] = ticker\n",
    "    return(collected_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_everything(ticker):\n",
    "    #Create the urls\n",
    "    url_statistics = f'https://finance.yahoo.com/quote/{ticker}/key-statistics?p={ticker}'\n",
    "    url_balance = f'https://finance.yahoo.com/quote/{ticker}/balance-sheet?p={ticker}'\n",
    "    url_flow = f'https://finance.yahoo.com/quote/{ticker}/cash-flow?p={ticker}'\n",
    "    url_financials = f'https://finance.yahoo.com/quote/{ticker}/financials?p={ticker}'\n",
    "    url_historical = f'https://finance.yahoo.com/quote/{ticker}/history?p={ticker}'\n",
    "\n",
    "\n",
    "    driver.get(url_financials)\n",
    "    time.sleep(2)\n",
    "    driver.find_element(By.XPATH, '//*[@id=\"Col1-1-Financials-Proxy\"]/section/div[1]/div[2]/button/div/span').click()\n",
    "    time.sleep(0.5)\n",
    "    driver.find_element(By.XPATH, '//*[@id=\"Col1-1-Financials-Proxy\"]/section/div[1]/div[2]/span/button').click()\n",
    "    time.sleep(1)\n",
    "    driver.get(url_balance)\n",
    "    time.sleep(2)\n",
    "    driver.find_element(By.XPATH, '//*[@id=\"Col1-1-Financials-Proxy\"]/section/div[1]/div[2]/button/div/span').click()\n",
    "    time.sleep(0.5)\n",
    "    driver.find_element(By.XPATH, '//*[@id=\"Col1-1-Financials-Proxy\"]/section/div[1]/div[2]/span/button').click()\n",
    "    time.sleep(1)\n",
    "    driver.get(url_flow)\n",
    "    time.sleep(2)\n",
    "    driver.find_element(By.XPATH, '//*[@id=\"Col1-1-Financials-Proxy\"]/section/div[1]/div[2]/button/div/span').click()\n",
    "    time.sleep(0.5)\n",
    "    driver.find_element(By.XPATH, '//*[@id=\"Col1-1-Financials-Proxy\"]/section/div[1]/div[2]/span/button').click()\n",
    "    time.sleep(1)\n",
    "    driver.get(url_statistics)\n",
    "    time.sleep(2)\n",
    "    driver.find_element(By.XPATH, '//*[@id=\"Col1-0-KeyStatistics-Proxy\"]/section/div[2]/div[1]/div[1]/span/button').click()\n",
    "    time.sleep(1)\n",
    "    driver.get(url_historical)\n",
    "    time.sleep(2)\n",
    "    driver.find_element(By.XPATH, '//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[1]/div[1]/div[1]/div').click()\n",
    "    time.sleep(0.5)\n",
    "    driver.find_element(By.XPATH, '//*[@id=\"dropdown-menu\"]/div/ul[2]/li[4]/button').click()\n",
    "    time.sleep(0.5)\n",
    "    driver.find_element(By.XPATH, '//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[1]/div[2]/span[2]/a').click()\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_list(list_of_stocks):\n",
    "    length = len(list_of_stocks)\n",
    "    counter = 0\n",
    "    counter2 = 0\n",
    "    for stock in list_of_stocks:\n",
    "        try:\n",
    "            download_everything(stock)\n",
    "        except:\n",
    "            print(f\"error downloading data for {stock}\")\n",
    "        counter += 1\n",
    "        counter2 += 1\n",
    "        print(f'{round((counter/length*100), 2)}% complete...')\n",
    "        if (counter2 == 10):\n",
    "            counter2 = 0\n",
    "            time.sleep(140)\n",
    "    print('All files downloaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_fluff(df):\n",
    "    for col_name in df.columns:\n",
    "        if (col_name[-8:]  == '.longFmt' or col_name[-4:] == '.fmt' or col_name[-2:] == '_x'):\n",
    "            del df[col_name]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_list(list_of_stocks):\n",
    "    temp = pd.DataFrame()\n",
    "    for stock in list_of_stocks:\n",
    "        single_stock_data = scrape(stock)\n",
    "        temp = pd.concat([temp, single_stock_data])\n",
    "        temp = remove_fluff(temp)\n",
    "    df = move_columns(['ticker', 'endDate'], temp)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_columns(list_of_columns, df):\n",
    "    for column in list_of_columns:\n",
    "        just_a_column = df[column]\n",
    "        df = df.drop(columns=[column])\n",
    "        df.insert(loc=0, column=column, value=just_a_column)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the driver\n",
    "# options = EdgeOptions()\n",
    "# options.use_chromium = True\n",
    "# driver = Edge(options=options)\n",
    "\n",
    "# url = 'https://finance.yahoo.com/'\n",
    "\n",
    "# driver.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the driver BUT THIS TIME in chrome\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "url = 'https://finance.yahoo.com/'\n",
    "\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.52% complete...\n",
      "3.03% complete...\n",
      "4.55% complete...\n",
      "6.06% complete...\n",
      "7.58% complete...\n",
      "9.09% complete...\n",
      "10.61% complete...\n",
      "12.12% complete...\n",
      "13.64% complete...\n",
      "15.15% complete...\n",
      "16.67% complete...\n",
      "18.18% complete...\n",
      "19.7% complete...\n",
      "21.21% complete...\n",
      "22.73% complete...\n",
      "24.24% complete...\n",
      "25.76% complete...\n",
      "27.27% complete...\n",
      "28.79% complete...\n",
      "30.3% complete...\n",
      "31.82% complete...\n",
      "33.33% complete...\n",
      "34.85% complete...\n",
      "36.36% complete...\n",
      "37.88% complete...\n",
      "39.39% complete...\n",
      "40.91% complete...\n",
      "42.42% complete...\n",
      "43.94% complete...\n",
      "45.45% complete...\n",
      "46.97% complete...\n",
      "48.48% complete...\n",
      "50.0% complete...\n",
      "51.52% complete...\n",
      "53.03% complete...\n",
      "54.55% complete...\n",
      "56.06% complete...\n",
      "57.58% complete...\n",
      "59.09% complete...\n",
      "60.61% complete...\n",
      "62.12% complete...\n",
      "63.64% complete...\n",
      "65.15% complete...\n",
      "66.67% complete...\n",
      "68.18% complete...\n",
      "69.7% complete...\n",
      "71.21% complete...\n",
      "72.73% complete...\n",
      "74.24% complete...\n",
      "75.76% complete...\n",
      "77.27% complete...\n",
      "78.79% complete...\n",
      "80.3% complete...\n",
      "81.82% complete...\n",
      "83.33% complete...\n",
      "84.85% complete...\n",
      "86.36% complete...\n",
      "87.88% complete...\n",
      "89.39% complete...\n",
      "90.91% complete...\n",
      "92.42% complete...\n",
      "93.94% complete...\n",
      "95.45% complete...\n",
      "96.97% complete...\n",
      "98.48% complete...\n",
      "100.0% complete...\n",
      "All files downloaded!\n"
     ]
    }
   ],
   "source": [
    "stocks = pd.read_csv('Tickers_remaining4.csv')\n",
    "\n",
    "stocks = stocks['Stocks'].tolist()\n",
    "\n",
    "download_list(stocks)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc816d8e426fe616fe65e3e533b3a5ac980db8a3cd9a2aa4e549e9c05f63913d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
